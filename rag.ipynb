{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for university courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We install dependecies and set up async support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uq llama-index llama-index-llms-groq llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the API key for Groq (the LLM) and we set up 2 clients: one with 8b prameters, more basic, and one with 70b parameters, more advanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.groq import Groq\n",
    "\n",
    "llm = Groq(model=\"llama3-8b-8192\")\n",
    "llm_70b = Groq(model=\"llama3-70b-8192\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the embedding model. An embedding model is a model that takes a list of strings and returns a list of vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0edec1973620448da7748a7e9e3aaa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef8d1a080a549b4a493c3c5c04a3f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6cb0f8d1961439fb6c636fbba530ddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0949638dd17347f9940be666dcc4bed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9422ae59e6d64bfbb8d14b77bb47b8e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e222fe63f3a54a82bb10de62098c6595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a42ba49aa34289b23aff7e3dc576ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cab351250d54e1d934dbb7e8a3ba323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45c4887ac6614026a86dd167663610c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25911094779844b9964172edfb4091c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11165b32233d4f3fba27dd6203ac3974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-m3\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We enable globally the llm and the embedding model to subsitute the OpenAI default ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried to load the data using the SimpleDirectoryReader however most of the data was splitted not optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "Doc ID: 996d6684-7235-4de8-87ed-88d58172ce7d\n",
      "Text: Orientamento ------------  * [I\n",
      "nostriÂ studenti](http://orienta.unitn.it/cosa-scegliere/56/beni-\n",
      "culturali) * [Eventi di orientamento](http://orienta.unitn.it/come-\n",
      "scegliere/4/eventi-di-orientamento) * [Orienta: tutti i\n",
      "servizi](http://orienta.unitn.it) * [Scegliere\n",
      "UniTrento](http://www.unitn.it/ateneo/4/perche-scegliere-unitrento)\n",
      "* Livello...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(\"documents\").load_data()\n",
    "\n",
    "print(len(documents))\n",
    "print(documents[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infact the SimpleDirectoryReader makes impossible to customize some essential metadata as chunks can be similar one from each other. We are going to load each document separately and parse manually the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['unitn-computer-science.md', 'unitn-industrial-engineering.md', 'unitn-mathematics.md']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_paths = []\n",
    "for x in os.listdir(\"eng\"):\n",
    "    if x.endswith(\".md\"):\n",
    "        file_paths.append(x)\n",
    "\n",
    "print(file_paths[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WWhile loading the data we are going to add some initial metadata to the documents. Specifically the university and the course the document is about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0/3\n",
      "Idx 1/3\n",
      "Idx 2/3\n",
      "{'university': 'unitn', 'course': 'computer science'}\n",
      "# Bachelor's Degree in Computer Science - University of Trento\n",
      "\n",
      "## Program Overview\n",
      "\n",
      "- **Level**: Bachelor's Degree (First Cycle)\n",
      "- **Duration**: 3 years\n",
      "- **Degree Class**: L-31 - Computer Science and Technologies\n",
      "- **Language**: Offered in **Italian** and **English**\n",
      "- **Admission**: **Limited enrollment**, requires passing an admission test\n",
      "- **Location**: Department of Information Engineering and Computer Science, Via Sommarive 5, 38123 Povo (TN), Italy\n",
      "\n",
      "## About the Program\n",
      "\n",
      "Computer Science at the University of Trento integrates elements from **Science** and **Engineering**:\n",
      "- From **Science**, it inherits **curiosity**, such as exploring philosophical aspects of problem-solving.\n",
      "- From **Engineering**, it inherits **methodological rigor** in solving problems.\n",
      "\n",
      "### Key Features\n",
      "- Recognized as one of the foundational pillars of modern sciences, alongside **theory** and **experimentation**.\n",
      "- Highly **pervasive** in modern society and a major contributor to economic growth.\n",
      "- Combines **theoretical foundations** and **practical tools** to enable industrial-grade software development.\n",
      "\n",
      "### Strengths of the Program\n",
      "- **Balanced Curriculum**: Combines theoretical and laboratory courses throughout the program.\n",
      "- **Job Placement**: Graduates find employment within 2 months on average (source: Almalaurea).\n",
      "- **Prestigious Recognition**: Received the **GRIN Quality Certification** for course content excellence.\n",
      "- **International Focus**: 20% of faculty and 40% of master's students are international, with extensive Erasmus and exchange opportunities.\n",
      "- **Strong Industry Links**: Regular industry events like **ICT Days** offer networking and recruitment opportunities.\n",
      "- **Modern Curriculum**: Emphasizes both foundational knowledge and cutting-edge technologies.\n",
      "\n",
      "## Learning Outcomes\n",
      "\n",
      "Graduates will acquire:\n",
      "- **Analytical Skills**: Ability to analyze complex systems (organizations, services, artificial systems).\n",
      "- **Problem-Solving Expertise**: Identifying key processes and proposing implementable models.\n",
      "- **Adaptability**: Versatility to adapt to rapid innovation and diverse work environments.\n",
      "\n",
      "## Career Prospects\n",
      "\n",
      "Graduates can:\n",
      "- Enter the workforce directly with a solid technical foundation in areas such as **networks**, **systems**, and **data management**.\n",
      "- Pursue **management roles** with advanced studies, combining problem-solving skills with leadership and innovation.\n",
      "- Leverage opportunities in fast-growing sectors where startups often outpace established firms.\n",
      "\n",
      "### Potential Roles\n",
      "- **Programmers**\n",
      "- **Network and System Administrators**\n",
      "- **Database Administrators**\n",
      "- **Application Developers**\n",
      "\n",
      "## Further Studies\n",
      "\n",
      "Graduates can continue their education with master's programs, including:\n",
      "- **Computer Science**\n",
      "- **Information Engineering**\n",
      "- **Artificial Intelligence Systems**\n",
      "- **Data Science**\n",
      "- **Human-Computer Interaction**\n",
      "- **Quantitative and Computational Biology**\n",
      "- Related fields like **Mathematics**, **Economics**, and **Management**.\n",
      "\n",
      "## Curriculum Structure\n",
      "\n",
      "### Core Courses (Compulsory - 180 Credits Total)\n",
      "| Course | Credits (CFU) |\n",
      "| --- | --- |\n",
      "| Mathematical Analysis 1 | 12 |\n",
      "| Geometry and Linear Algebra | 6 |\n",
      "| Programming 1 | 12 |\n",
      "| Computer Architecture | 6 |\n",
      "| Mathematical Foundations for Computer Science | 6 |\n",
      "| Probability and Statistics | 6 |\n",
      "| Programming 2 | 6 |\n",
      "| Functional Programming | 6 |\n",
      "| Algorithms and Data Structures | 12 |\n",
      "| Software Engineering | 12 |\n",
      "| Databases | 6 |\n",
      "| Networks | 6 |\n",
      "| Operating Systems | 12 |\n",
      "| Computational Logic | 6 |\n",
      "| Physics | 6 |\n",
      "| Formal Languages and Compilers | 12 |\n",
      "| Introduction to Machine Learning | 6 |\n",
      "\n",
      "### Elective Courses (Choose 12 Credits)\n",
      "| Course | Credits (CFU) |\n",
      "| --- | --- |\n",
      "| Introduction to Web Programming | 6 |\n",
      "| Human-Computer Interaction (English) | 6 |\n",
      "| Logic Circuits | 6 |\n",
      "| Signal Processing Basics | 6 |\n",
      "| Computer and Network Security (English) | 6 |\n",
      "| Information Systems | 6 |\n",
      "| Advanced Programming | 6 |\n",
      "| Mobile and Tablet Systems Programming Lab | 6 |\n",
      "| Optimization Techniques (English) | 6 |\n",
      "| Mathematical Analysis 2 | 6 |\n",
      "| Free Electives | 12 |\n",
      "\n",
      "### Additional Requirements\n",
      "| Activity | Credits (CFU) |\n",
      "| --- | --- |\n",
      "| Internship | 9 |\n",
      "| English Language B1 Level | 3 |\n",
      "| Final Thesis | 6 |\n",
      "\n",
      "## Admissions for Academic Year 2025/2026\n",
      "\n",
      "### Admission Requirements\n",
      "- High school diploma or equivalent qualification recognized internationally.\n",
      "- Admission test (**TOLC**) organized by **CISIA** (Interuniversity Consortium for Integrated Access Systems).\n",
      "\n",
      "### Application Process\n",
      "- Details, deadlines, and requirements are published on the [2025 Admissions Portal](https://www.unitn.it/ammissioni-2025).\n",
      "- Test preparation resources and simulations are available on the [CISIA Website](https://www.cisiaonline.it/area-tematica-tolc-cisia/tolc-esercitazioni-e-simulazioni/).\n",
      "\n",
      "### Transfers and Credit Recognition\n",
      "- Applications for transfers or credit recognition (e.g., from completed degrees or discontinued studies) must be submitted by **February 3, 2025**.\n",
      "- Available only to EU citizens and non-EU citizens already residing in Italy.\n",
      "\n",
      "## Contact Information\n",
      "For further details, visit the [University of Trento Official Website](https://www.unitn.it) or contact the Department of Information Engineering and Computer Science.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "documents = []\n",
    "for idx, f in enumerate(file_paths):\n",
    "    print(f\"Idx {idx}/{len(file_paths)}\")\n",
    "    content = open(f\"eng/{f}\", \"r\").read()\n",
    "    loaded_doc = Document(\n",
    "        text=content,\n",
    "        metadata={\"university\": str(f.split(\"-\")[0]), \"course\": str(\" \".join(f.split(\"-\")[1:]).split(\".\")[0])},\n",
    "\n",
    "    )\n",
    "    documents.append(loaded_doc)\n",
    "\n",
    "print(documents[0].metadata)\n",
    "print(documents[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing is the process to assign documents to vectors. We are going to use the embedding model to assign a vector to each document. We also use MarkdownNodeParser that:\n",
    "\n",
    "1. Split each md document in sections following the headings\n",
    "2. Extract other metadata from the document\n",
    "3. Link documents near each other\n",
    "\n",
    "We then create a vector store and index the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b1ac460da24046a335c8f3345f6674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70833d0cb4464af39b1864a20ac5a6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.core.extractors import (\n",
    "    QuestionsAnsweredExtractor,\n",
    "    KeywordExtractor,\n",
    ")\n",
    "\n",
    "parser = MarkdownNodeParser(\n",
    "        include_prev_next_rel=True,\n",
    "        include_metadata=True,\n",
    "    )\n",
    "\n",
    "# Other possible metadata extractors to use however requires LLM calls and it reaches the limit\n",
    "\n",
    "# question_extractor = QuestionsAnsweredExtractor(\n",
    "#     questions=3\n",
    "# )\n",
    "\n",
    "# keyword_extractor = KeywordExtractor(\n",
    "#     keywords=10\n",
    "# )\n",
    "\n",
    "# We may also save the index to disk\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    transformations=[parser],\n",
    "    show_progress=True,\n",
    ")\n",
    "\n",
    "query_engine = index.as_query_engine(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having the query_engine we can now query the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mathematical Analysis A1, Mathematical Analysis A2, Geometry A, Algebra A, Computer Science, English B1.\n"
     ]
    }
   ],
   "source": [
    "res = query_engine.query(\"What are the first year courses of mathematics?\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# show one node for debug\n",
    "nodes = index.docstore.docs\n",
    "j = json.dumps(str(nodes))\n",
    "# select a random key and return the content\n",
    "\n",
    "# save nodes to file index.json\n",
    "with open(\"nodes.json\", \"w\") as f:\n",
    "    f.write(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you give me two short (max 20 words) questions which answer is contained in the following text? Please include in the questions as much context as you can. Give me also the answers. Use JSON format such as: [{question: '[QUESTION]', answer: '[ANSWER]' }, ...].\\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = []\n",
    "for x in os.listdir(\"documents\"):\n",
    "    if x.endswith(\".md\"):\n",
    "        files.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Calculated available context size -1317 was not non-negative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m      4\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m----> 5\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mquery_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCan you give me two short (max 20 words) questions which answer is contained in the following text? Please include in the questions as much context as you can. Give me also the answers. Return just this JSON format such as: [\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43mquestion: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[QUESTION]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m, answer: \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m[ANSWER]\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m }, ...].\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# write to file\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res\u001b[38;5;241m.\u001b[39mresponse)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/base/base_query_engine.py:52\u001b[0m, in \u001b[0;36mBaseQueryEngine.query\u001b[0;34m(self, str_or_query_bundle)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(str_or_query_bundle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     51\u001b[0m         str_or_query_bundle \u001b[38;5;241m=\u001b[39m QueryBundle(str_or_query_bundle)\n\u001b[0;32m---> 52\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstr_or_query_bundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m dispatcher\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m     54\u001b[0m     QueryEndEvent(query\u001b[38;5;241m=\u001b[39mstr_or_query_bundle, response\u001b[38;5;241m=\u001b[39mquery_result)\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_result\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py:179\u001b[0m, in \u001b[0;36mRetrieverQueryEngine._query\u001b[0;34m(self, query_bundle)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    176\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mQUERY, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query_bundle\u001b[38;5;241m.\u001b[39mquery_str}\n\u001b[1;32m    177\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m query_event:\n\u001b[1;32m    178\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve(query_bundle)\n\u001b[0;32m--> 179\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_response_synthesizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msynthesize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_bundle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m     query_event\u001b[38;5;241m.\u001b[39mon_end(payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mRESPONSE: response})\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/base.py:241\u001b[0m, in \u001b[0;36mBaseSynthesizer.synthesize\u001b[0;34m(self, query, nodes, additional_source_nodes, **response_kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m     query \u001b[38;5;241m=\u001b[39m QueryBundle(query_str\u001b[38;5;241m=\u001b[39mquery)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_callback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    238\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mSYNTHESIZE,\n\u001b[1;32m    239\u001b[0m     payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mQUERY_STR: query\u001b[38;5;241m.\u001b[39mquery_str},\n\u001b[1;32m    240\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 241\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_str\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMetadataMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnodes\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     additional_source_nodes \u001b[38;5;241m=\u001b[39m additional_source_nodes \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[1;32m    250\u001b[0m     source_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(nodes) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(additional_source_nodes)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py:321\u001b[0m, in \u001b[0;36mDispatcher.span.<locals>.wrapper\u001b[0;34m(func, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m             _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to reset active_span_id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 321\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, asyncio\u001b[38;5;241m.\u001b[39mFuture):\n\u001b[1;32m    323\u001b[0m         \u001b[38;5;66;03m# If the result is a Future, wrap it\u001b[39;00m\n\u001b[1;32m    324\u001b[0m         new_future \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(result)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:42\u001b[0m, in \u001b[0;36mCompactAndRefine.get_response\u001b[0;34m(self, query_str, text_chunks, prev_response, **response_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get compact response.\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# use prompt helper to fix compact text_chunks under the prompt limitation\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# TODO: This is a temporary fix - reason it's temporary is that\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# the refine template does not account for size of previous answer.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m new_texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_compact_text_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mget_response(\n\u001b[1;32m     44\u001b[0m     query_str\u001b[38;5;241m=\u001b[39mquery_str,\n\u001b[1;32m     45\u001b[0m     text_chunks\u001b[38;5;241m=\u001b[39mnew_texts,\n\u001b[1;32m     46\u001b[0m     prev_response\u001b[38;5;241m=\u001b[39mprev_response,\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresponse_kwargs,\n\u001b[1;32m     48\u001b[0m )\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/compact_and_refine.py:57\u001b[0m, in \u001b[0;36mCompactAndRefine._make_compact_text_chunks\u001b[0;34m(self, query_str, text_chunks)\u001b[0m\n\u001b[1;32m     54\u001b[0m refine_template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_refine_template\u001b[38;5;241m.\u001b[39mpartial_format(query_str\u001b[38;5;241m=\u001b[39mquery_str)\n\u001b[1;32m     56\u001b[0m max_prompt \u001b[38;5;241m=\u001b[39m get_biggest_prompt([text_qa_template, refine_template])\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prompt_helper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_llm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:292\u001b[0m, in \u001b[0;36mPromptHelper.repack\u001b[0;34m(self, prompt, text_chunks, padding, llm, tools)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrepack\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     tools: Optional[List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseTool\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    285\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    286\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Repack text chunks to fit available context window.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    This will combine text chunks into consolidated chunks\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m    that more fully \"pack\" the prompt template given the context_window.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m     text_splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text_splitter_given_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    295\u001b[0m     combined_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([c\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m text_chunks \u001b[38;5;28;01mif\u001b[39;00m c\u001b[38;5;241m.\u001b[39mstrip()])\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text_splitter\u001b[38;5;241m.\u001b[39msplit_text(combined_str)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:247\u001b[0m, in \u001b[0;36mPromptHelper.get_text_splitter_given_prompt\u001b[0;34m(self, prompt, num_chunks, padding, llm, tools)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_text_splitter_given_prompt\u001b[39m(\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    238\u001b[0m     prompt: BasePromptTemplate,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     tools: Optional[List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseTool\u001b[39m\u001b[38;5;124m\"\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    243\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TokenTextSplitter:\n\u001b[1;32m    244\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get text splitter configured to maximally pack available context window,\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    taking into account of given prompt, and desired number of chunks.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m     chunk_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_chunk_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunk_size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not positive.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:230\u001b[0m, in \u001b[0;36mPromptHelper._get_available_chunk_size\u001b[0;34m(self, prompt, num_chunks, padding, llm, tools)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m llm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     num_prompt_tokens \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_counter\u001b[38;5;241m.\u001b[39mget_string_tokens(\n\u001b[1;32m    227\u001b[0m         llm\u001b[38;5;241m.\u001b[39msystem_prompt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 230\u001b[0m available_context_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_available_context_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_prompt_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m result \u001b[38;5;241m=\u001b[39m available_context_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_chunks \u001b[38;5;241m-\u001b[39m padding\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_limit \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Dev/myVision-RAG/.venv/lib/python3.12/site-packages/llama_index/core/indices/prompt_helper.py:153\u001b[0m, in \u001b[0;36mPromptHelper._get_available_context_size\u001b[0;34m(self, num_prompt_tokens)\u001b[0m\n\u001b[1;32m    151\u001b[0m context_size_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_window \u001b[38;5;241m-\u001b[39m num_prompt_tokens \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_output\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context_size_tokens \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculated available context size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext_size_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not non-negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m context_size_tokens\n",
      "\u001b[0;31mValueError\u001b[0m: Calculated available context size -1317 was not non-negative."
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "\n",
    "for file in files:\n",
    "    content = open(f\"documents/{file}\").read()\n",
    "    res = query_engine.query(\"Can you give me two short (max 20 words) questions which answer is contained in the following text? Please include in the questions as much context as you can. Give me also the answers. Return just this JSON format such as: [{question: '[QUESTION]', answer: '[ANSWER]' }, ...].\\n\\n\" + content)\n",
    "    # write to file\n",
    "    print(res.response)\n",
    "    questions.append(res.response)\n",
    "\n",
    "print(questions[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for each question in q.json file, we try to answer it using the LLM model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
